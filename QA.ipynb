{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "dfTmYMZ36i9-",
        "outputId": "c48ac8e7-a648-4310-d428-d2dbb7eeb07e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1126' max='1126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1126/1126 24:18, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.299900</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.199800</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1126, training_loss=4.684278362809447, metrics={'train_runtime': 1459.5067, 'train_samples_per_second': 12.344, 'train_steps_per_second': 0.771, 'total_flos': 4740166268289024.0, 'train_loss': 4.684278362809447, 'epoch': 2.0})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "ds = load_dataset(\"SajjadAyoubi/persian_qa\")\n",
        "\n",
        "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "\n",
        "        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø§Ø²Ù… Ø¯Ø± Ù¾Ø§Ø³Ø®\n",
        "        if len(answer[\"answer_start\"]) == 0 or len(answer[\"text\"]) == 0:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "            continue\n",
        "\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer[\"text\"][0])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø­Ø¯ÙˆØ¯Ù‡â€ŒÛŒ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¯Ø± ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§\n",
        "        context_start = 0\n",
        "        while context_start < len(sequence_ids) and sequence_ids[context_start] != 1:\n",
        "            context_start += 1\n",
        "\n",
        "        context_end = len(sequence_ids) - 1\n",
        "        while context_end >= 0 and sequence_ids[context_end] != 1:\n",
        "            context_end -= 1\n",
        "\n",
        "        if context_start >= len(sequence_ids) or context_end < 0 or offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            start_position = None\n",
        "            end_position = None\n",
        "            for j, (start, end) in enumerate(offset):\n",
        "                if start <= start_char < end:\n",
        "                    start_position = j\n",
        "                if start < end_char <= end:\n",
        "                    end_position = j\n",
        "                    break\n",
        "            start_positions.append(start_position if start_position is not None else 0)\n",
        "            end_positions.append(end_position if end_position is not None else 0)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "tokenized_ds = ds.map(preprocess_function, batched=True, remove_columns=ds[\"train\"].column_names)\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/temp/result\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=lora_model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_ds[\"train\"],\n",
        "   eval_dataset=tokenized_ds[\"validation\"],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BtHWZQ1g9dnx"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/temp/model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/temp/tokenizer\")\n",
        "\n",
        "# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ LoRA\n",
        "lora_model.save_pretrained(\"/content/drive/MyDrive/temp/model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "USds6y7BDiEG",
        "outputId": "9751699e-c238-455e-87ac-a581026b29f2"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_05da2a50-69fa-4a03-86d1-0dd0433657a2\", \"model.zip\", 604476890)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f3f12131-29db-4041-b4ff-2f26cffc0ca2\", \"tokenizer.zip\", 1228865)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÙˆØ´Ù‡ Ù…Ø¯Ù„\n",
        "shutil.make_archive('/content/model', 'zip', '/content/drive/MyDrive/temp/model')\n",
        "shutil.make_archive('/content/tokenizer', 'zip', '/content/drive/MyDrive/temp/tokenizer')\n",
        "\n",
        "# Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙØ´Ø±Ø¯Ù‡ Ø´Ø¯Ù‡\n",
        "files.download('/content/model.zip')\n",
        "files.download('/content/tokenizer.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-koml5KDuDC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
